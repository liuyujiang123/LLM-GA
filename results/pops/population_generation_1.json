[
     {
          "algorithm": "```",
          "code": "import torch\nfrom torch.autograd import Variable\n\ndef gen_adv_examples(self, org_img, model, target):\n    adv_img = Variable(org_img.data, requires_grad=True)\n    adv_noise = torch.zeros_like(org_img)\n    for _ in range(self.maxiter):\n        grad = self.get_gradient(adv_img, model, target)\n        grad = grad / (torch.norm(grad, p=2) + 1e-8)\n        adv_noise = self.alpha * grad + adv_noise\n        adv_img = adv_img + self.alpha * adv_noise.sign()\n        adv_img = self.clip_adv(org_img, adv_img)\n    return adv_img",
          "objective": 564.94322,
          "other_inf": null
     },
     {
          "algorithm": "```",
          "code": "import torch\nfrom torch.autograd import Variable\n\ndef gen_adv_examples(self, org_img, model, target):\n    adv_img = Variable(org_img.data, requires_grad=True)\n    eta = torch.zeros_like(org_img)\n    for _ in range(self.maxiter):\n        grad = self.get_gradient(adv_img, model, target)\n        eta = eta + grad / torch.norm(grad, p=2, dim=[1, 2, 3], keepdim=True)\n        adv_img = adv_img + self.alpha * eta.sign()\n        adv_img = self.clip_adv(org_img, adv_img)\n    return adv_img",
          "objective": 568.01817,
          "other_inf": null
     },
     {
          "algorithm": "```",
          "code": "import torch\nfrom torch.autograd import Variable\n\ndef gen_adv_examples(self, org_img, model, target):\n    adv_img = Variable(org_img.data, requires_grad=True)\n    momentum = 0\n    decay_factor = 1.0\n    \n    for _ in range(self.maxiter):\n        grad = self.get_gradient(adv_img, model, target)\n        momentum = decay_factor * momentum + grad / torch.norm(grad, p=1, dim=[1, 2, 3], keepdim=True)\n        adv_img = adv_img + self.alpha * momentum.sign()\n        adv_img = self.clip_adv(org_img, adv_img)\n        \n    return adv_img",
          "objective": 590.52085,
          "other_inf": null
     },
     {
          "algorithm": "This algorithm implements a momentum-based iterative attack with a decaying normalization factor to generate adversarial examples.",
          "code": "import torch\nfrom torch.autograd import Variable\n\ndef gen_adv_examples(self, org_img, model, target):\n    \"\"\"\n    {This algorithm implements a momentum-based iterative attack with a decaying normalization factor to generate adversarial examples.}\n    \"\"\"\n    adv_img = Variable(org_img.data, requires_grad=True)\n    momentum = torch.zeros_like(org_img)\n    decay_factor = 0.9  # Decaying factor for momentum\n\n    for _ in range(self.maxiter):\n        grad = self.get_gradient(adv_img, model, target)\n        grad = grad / (torch.norm(grad, p=2) + 1e-8)  # Using L2 normalization\n        momentum = decay_factor * momentum + grad\n        adv_img = adv_img + self.alpha * momentum.sign()\n        adv_img = self.clip_adv(org_img, adv_img)\n\n    return adv_img",
          "objective": 706.97195,
          "other_inf": null
     },
     {
          "algorithm": "Generates adversarial examples by iteratively perturbing the input images using momentum to stabilize the direction of gradient ascent, ensuring convergence towards the target label while maintaining visual similarity.",
          "code": "import torch\nfrom torch.autograd import Variable\n\ndef gen_adv_examples(self, org_img, model, target):\n    \"\"\"{Generates adversarial examples by iteratively perturbing the input images using momentum to stabilize the direction of gradient ascent, ensuring convergence towards the target label while maintaining visual similarity.}\"\"\"\n    g = torch.zeros_like(org_img)\n    adv_img = Variable(org_img.data, requires_grad=True)\n    for _ in range(self.maxiter):\n        grad = self.get_gradient(adv_img, model, target)\n        g = 0.9 * g + grad / torch.norm(grad, p=1, dim=[1, 2, 3], keepdim=True)\n        adv_img = adv_img + self.alpha * g.sign()\n        adv_img = self.clip_adv(org_img, adv_img)\n    return adv_img",
          "objective": 707.7095,
          "other_inf": null
     },
     {
          "algorithm": "```",
          "code": "import torch\nfrom torch.autograd import Variable\n\ndef gen_adv_examples(self, org_img, model, target):\n    adv_img = Variable(org_img.data, requires_grad=True)\n    for _ in range(self.maxiter):\n        grad = self.get_gradient(adv_img, model, target)\n        adv_img = adv_img + self.alpha * grad.sign()\n        adv_img = self.clip_adv(org_img, adv_img)\n    return adv_img",
          "objective": 912.11952,
          "other_inf": null
     },
     {
          "algorithm": "```",
          "code": "import torch\nfrom torch.autograd import Variable\n\ndef gen_adv_examples(self, org_img, model, target):\n    adv_img = Variable(org_img.data, requires_grad=True)\n    momentum = torch.zeros_like(org_img)\n    for _ in range(self.maxiter):\n        grad = self.get_gradient(adv_img, model, target)\n        grad = grad / (torch.norm(grad, p=1) + 1e-8)\n        momentum = self.alpha * momentum + grad\n        adv_img = adv_img + self.alpha * momentum.sign()\n        adv_img = self.clip_adv(org_img, adv_img)\n    return adv_img",
          "objective": 930.60526,
          "other_inf": null
     },
     {
          "algorithm": "Generates adversarial examples by iteratively perturbing the input images using gradient ascent to approach the target label while ensuring visual similarity.",
          "code": "import torch\nfrom torch.autograd import Variable\n\ndef gen_adv_examples(self, org_img, model, target):\n    \"\"\"{Generates adversarial examples by iteratively perturbing the input images using gradient ascent to approach the target label while ensuring visual similarity.}\"\"\"\n    adv_img = Variable(org_img.data, requires_grad=True)\n    for _ in range(self.maxiter):\n        grad = self.get_gradient(adv_img, model, target)\n        adv_img = adv_img + self.alpha * grad.sign()\n        adv_img = self.clip_adv(org_img, adv_img)\n    return adv_img",
          "objective": 932.13392,
          "other_inf": null
     }
]