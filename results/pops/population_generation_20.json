[
     {
          "algorithm": "This algorithm employs a multi-scale gradient accumulation technique with adaptive step sizes, ensuring the generation of high-confidence adversarial examples while maintaining visual similarity to the original images.",
          "code": "import torch\nfrom torch.autograd import Variable\n\ndef gen_adv_examples(self, org_img, model, target):\n    \"\"\"\n    {This algorithm employs a multi-scale gradient accumulation technique with adaptive step sizes, ensuring the generation of high-confidence adversarial examples while maintaining visual similarity to the original images.}\n    \"\"\"\n    adv_img = Variable(org_img.data, requires_grad=True)\n    accumulated_grad = torch.zeros_like(org_img)\n    step_size = self.alpha\n\n    for _ in range(self.maxiter):\n        grad = self.get_gradient(adv_img, model, target)\n        accumulated_grad += grad / (torch.norm(grad, p=2) + 1e-8)\n        \n        # Calculate a dynamic step size based on the current accumulated gradient\n        dynamic_step_size = step_size / (1 + torch.norm(accumulated_grad, p=2))\n        \n        adv_img = adv_img + dynamic_step_size * accumulated_grad.sign()\n        adv_img = self.clip_adv(org_img, adv_img)\n\n    return adv_img",
          "objective": 363.02705,
          "other_inf": null
     },
     {
          "algorithm": "This algorithm uses a momentum-based approach, where the gradient is smoothed over iterations to stabilize the update direction, while a decaying step size ensures subtle perturbations.",
          "code": "import torch\nfrom torch.autograd import Variable\n\ndef gen_adv_examples(self, org_img, model, target):\n    \"\"\"\n    {This algorithm uses a momentum-based approach, where the gradient is smoothed over iterations to stabilize the update direction, while a decaying step size ensures subtle perturbations.}\n    \"\"\"\n    adv_img = Variable(org_img.data, requires_grad=True)\n    momentum = torch.zeros_like(org_img)\n    step_size = self.alpha\n\n    for _ in range(self.maxiter):\n        grad = self.get_gradient(adv_img, model, target)\n        momentum = 0.9 * momentum + grad / (torch.norm(grad, p=2) + 1e-8)\n        \n        # Decaying step size based on iteration\n        decay_factor = step_size / (1 + _)\n        \n        adv_img = adv_img + decay_factor * momentum.sign()\n        adv_img = self.clip_adv(org_img, adv_img)\n\n    return adv_img",
          "objective": 375.27383,
          "other_inf": null
     },
     {
          "algorithm": "This algorithm utilizes a momentum-based gradient ascent method with an exponential decay applied to the step size, aiming to efficiently guide the adversarial examples towards the target label while maintaining their perceptual similarity to the original images.",
          "code": "import torch\nfrom torch.autograd import Variable\n\ndef gen_adv_examples(self, org_img, model, target):\n    \"\"\"\n    {This algorithm utilizes a momentum-based gradient ascent method with an exponential decay applied to the step size, aiming to efficiently guide the adversarial examples towards the target label while maintaining their perceptual similarity to the original images.}\n    \"\"\"\n    adv_img = Variable(org_img.data, requires_grad=True)\n    momentum = torch.zeros_like(org_img)\n    step_size = self.alpha\n\n    for _ in range(self.maxiter):\n        grad = self.get_gradient(adv_img, model, target)\n        grad /= (torch.norm(grad, p=2) + 1e-8)\n        momentum = momentum * 0.9 + grad\n        adv_img = adv_img + step_size * momentum.sign()\n        adv_img = self.clip_adv(org_img, adv_img)\n        step_size *= 0.9  # Exponential decay of step size\n\n    return adv_img",
          "objective": 383.15297,
          "other_inf": null
     },
     {
          "algorithm": "This algorithm employs a momentum-based approach where the gradient is accumulated using a momentum term to stabilize the direction of perturbations, while the perturbation magnitude is adaptively adjusted within a specified epsilon bound to ensure minimal visual distortion.",
          "code": "import torch\nfrom torch.autograd import Variable\n\ndef gen_adv_examples(self, org_img, model, target):\n    \"\"\"\n    {This algorithm employs a momentum-based approach where the gradient is accumulated using a momentum term to stabilize the direction of perturbations, while the perturbation magnitude is adaptively adjusted within a specified epsilon bound to ensure minimal visual distortion.}\n    \"\"\"\n    adv_img = Variable(org_img.data, requires_grad=True)\n    momentum = torch.zeros_like(org_img)\n    decay_factor = 1.0\n    epsilon = self.epsilon\n\n    for _ in range(self.maxiter):\n        grad = self.get_gradient(adv_img, model, target)\n        momentum = decay_factor * momentum + grad / (torch.norm(grad, p=1) + 1e-8)\n        perturbation = epsilon * momentum.sign()\n        adv_img = adv_img + self.alpha * perturbation\n        adv_img = self.clip_adv(org_img, adv_img)\n\n    return adv_img",
          "objective": 387.08707,
          "other_inf": null
     },
     {
          "algorithm": "This algorithm applies a momentum-based approach with a dynamic decay adjustment for the accumulated gradients, ensuring subtle perturbations while iteratively steering the model's predictions towards the target label.",
          "code": "import torch\nfrom torch.autograd import Variable\n\ndef gen_adv_examples(self, org_img, model, target):\n    \"\"\"\n    {This algorithm applies a momentum-based approach with a dynamic decay adjustment for the accumulated gradients, ensuring subtle perturbations while iteratively steering the model's predictions towards the target label.}\n    \"\"\"\n    adv_img = Variable(org_img.data, requires_grad=True)\n    momentum = torch.zeros_like(org_img)\n    decay_factor = 0.9\n    initial_step_size = self.alpha\n\n    for _ in range(self.maxiter):\n        grad = self.get_gradient(adv_img, model, target)\n        momentum = decay_factor * momentum + grad / (torch.norm(grad, p=1) + 1e-8)\n        adv_img = adv_img + initial_step_size * momentum.sign()\n        adv_img = self.clip_adv(org_img, adv_img)\n        initial_step_size *= 0.9  # Dynamic step size adjustment\n\n    return adv_img",
          "objective": 391.88192,
          "other_inf": null
     },
     {
          "algorithm": "This algorithm utilizes iterative gradient ascent with momentum and combines it with a decaying step size strategy to generate adversarial examples that are visually similar to the original images.",
          "code": "import torch\nfrom torch.autograd import Variable\n\ndef gen_adv_examples(self, org_img, model, target):\n    \"\"\"\n    {This algorithm utilizes iterative gradient ascent with momentum and combines it with a decaying step size strategy to generate adversarial examples that are visually similar to the original images.}\n    \"\"\"\n    momentum = 0\n    decay_factor = 0.9\n    adv_img = Variable(org_img.data, requires_grad=True)\n    step_size = self.alpha\n    for _ in range(self.maxiter):\n        grad = self.get_gradient(adv_img, model, target)\n        momentum = decay_factor * momentum + grad / (torch.norm(grad, p=float('inf')) + 1e-8)\n        adv_img = adv_img + step_size * momentum.sign()\n        adv_img = self.clip_adv(org_img, adv_img)\n        step_size *= 0.9  # Adaptive step size reduction\n\n    return adv_img",
          "objective": 422.78438,
          "other_inf": null
     },
     {
          "algorithm": "This algorithm uses a projected gradient descent approach with adaptive momentum scaling, employing L-infinity normalization to create adversarial examples.",
          "code": "import torch\nfrom torch.autograd import Variable\n\ndef gen_adv_examples(self, org_img, model, target):\n    \"\"\"\n    {This algorithm uses a projected gradient descent approach with adaptive momentum scaling, employing L-infinity normalization to create adversarial examples.}\n    \"\"\"\n    adv_img = Variable(org_img.data, requires_grad=True)\n    momentum = torch.zeros_like(org_img)\n    step_size = self.alpha\n\n    for _ in range(self.maxiter):\n        grad = self.get_gradient(adv_img, model, target)\n        grad = grad / (torch.norm(grad, p=float('inf')) + 1e-8)  # Using L-infinity normalization\n        momentum = 0.85 * momentum + grad  # Adaptive momentum scaling\n        adv_img = adv_img + step_size * momentum.sign()\n        adv_img = self.clip_adv(org_img, adv_img)\n        step_size = step_size * 0.9  # Adaptive step size reduction\n\n    return adv_img",
          "objective": 423.63354,
          "other_inf": null
     },
     {
          "algorithm": "This algorithm employs a gradient ascent approach where an accumulated weighted average of past gradients is used to stabilize updates, while the perturbation direction is adjusted using cosine similarity to ensure minimal visual distortion.",
          "code": "import torch\nfrom torch.autograd import Variable\n\ndef gen_adv_examples(self, org_img, model, target):\n    \"\"\"\n    {This algorithm employs a gradient ascent approach where an accumulated weighted average of past gradients is used to stabilize updates, while the perturbation direction is adjusted using cosine similarity to ensure minimal visual distortion.}\n    \"\"\"\n    adv_img = Variable(org_img.data, requires_grad=True)\n    accumulated_grad = torch.zeros_like(org_img)\n\n    for _ in range(self.maxiter):\n        grad = self.get_gradient(adv_img, model, target)\n        grad = grad / (torch.norm(grad, p=2) + 1e-8)\n        accumulated_grad = accumulated_grad + grad  # Accumulate gradients\n        cosine_similarity = (grad * accumulated_grad).sum() / (torch.norm(grad) * torch.norm(accumulated_grad) + 1e-8)\n        perturbation = self.alpha * cosine_similarity * accumulated_grad.sign()\n        adv_img = adv_img + perturbation\n        adv_img = self.clip_adv(org_img, adv_img)\n\n    return adv_img",
          "objective": 425.63573,
          "other_inf": null
     }
]