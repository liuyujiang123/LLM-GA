{
     "algorithm": "Utilize a cyclic learning rate to adjust the step size dynamically in a momentum-based iterative gradient attack to generate adversarial examples.",
     "code": "import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n# {Utilize a cyclic learning rate to adjust the step size dynamically in a momentum-based iterative gradient attack to generate adversarial examples.}\n\ndef gen_adv_examples(org_img, target_model, target):\n    maxiter = 30\n    epsilon = 0.3\n    alpha = 0.002\n    decay_factor = 1.0\n    loss_func = nn.BCELoss(reduction='mean')\n\n    # Random initialization within the epsilon ball\n    adv_img = org_img + torch.empty_like(org_img).uniform_(-epsilon, epsilon)\n    adv_img = Variable(adv_img.data, requires_grad=True)\n\n    # Initialize momentum\n    g = torch.zeros_like(org_img)\n\n    # Cyclic learning rate parameters\n    base_lr = alpha\n    max_lr = 0.01\n    step_size = 5\n\n    for iteration in range(maxiter):\n        # Adjust alpha using cyclic learning rate policy\n        cycle = iteration // (2 * step_size)\n        x = abs(iteration / step_size - 2 * cycle - 1)\n        lr = base_lr + (max_lr - base_lr) * max(0, (1 - x))\n\n        output = target_model(adv_img)\n        true_label = output.clone()\n        true_label[true_label >= 0.5] = 1\n        true_label[true_label < 0.5] = 0\n        loss = -loss_func(output, target.float())\n        target_model.zero_grad()\n\n        if adv_img.grad is not None:\n            adv_img.grad.data.zero_()\n        \n        loss.backward()\n        \n        # Normalize gradient using L2 norm\n        grad_norm = torch.norm(adv_img.grad, p=2, dim=[1, 2, 3], keepdim=True)\n        normalized_grad = adv_img.grad / (grad_norm + 1e-8)\n\n        # Update momentum\n        g = decay_factor * g + normalized_grad\n\n        # Update adversarial image\n        adv_img = adv_img + lr * g.sign()\n        adv_img = torch.clamp(adv_img, org_img - epsilon, org_img + epsilon)\n        adv_img = torch.clamp(adv_img, 0, 1)\n\n        adv_img = Variable(adv_img.data, requires_grad=True)\n\n    return adv_img",
     "objective": 225.52448,
     "other_inf": null
}