{
     "algorithm": "The algorithm utilizes a combination of adaptive momentum-based gradient accumulation and dynamic noise injection to progressively generate adversarial images with enhanced perturbations.",
     "code": "import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n# {The algorithm utilizes a combination of adaptive momentum-based gradient accumulation and dynamic noise injection to progressively generate adversarial images with enhanced perturbations.}\n\ndef gen_adv_examples(org_img, target_model, target):\n    maxiter = 30\n    epsilon = 0.3\n    alpha = 0.002\n    momentum = 0.9\n    noise_factor = 0.01\n    loss_func = nn.BCELoss(reduction='mean')\n\n    adv_img = org_img + torch.empty_like(org_img).uniform_(-epsilon, epsilon)\n    adv_img = torch.clamp(adv_img, 0, 1)\n    adv_img = Variable(adv_img.data, requires_grad=True)\n\n    momentum_grad = torch.zeros_like(org_img)\n\n    for iteration in range(maxiter):\n        output = target_model(adv_img)\n        loss = -loss_func(output, target.float())\n\n        if adv_img.grad is not None:\n            adv_img.grad.data.zero_()\n\n        loss.backward()\n\n        grad = adv_img.grad / (torch.norm(adv_img.grad, p=2, dim=[1, 2, 3], keepdim=True) + 1e-8)\n        grad = grad * (1 + noise_factor * torch.empty_like(grad).normal_())\n        momentum_grad = momentum * momentum_grad + grad\n\n        adv_img = adv_img + alpha * momentum_grad.sign()\n        adv_img = torch.clamp(adv_img, org_img - epsilon, org_img + epsilon)\n        adv_img = torch.clamp(adv_img, 0, 1)\n        adv_img = Variable(adv_img.data, requires_grad=True)\n\n    return adv_img",
     "objective": 222.71744,
     "other_inf": null
}