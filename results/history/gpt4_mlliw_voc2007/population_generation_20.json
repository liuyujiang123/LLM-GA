{
     "algorithm": "This algorithm utilizes a gradient-based approach with adaptive step size and a dynamic perturbation scaling strategy to generate adversarial examples targeting specific labels.",
     "code": "import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n# {This algorithm utilizes a gradient-based approach with adaptive step size and a dynamic perturbation scaling strategy to generate adversarial examples targeting specific labels.}\n\ndef gen_adv_examples(org_img, target_model, target):\n    maxiter = 30\n    epsilon = 0.3\n    alpha = 0.002\n    decay_factor = 0.85\n    loss_func = nn.BCELoss(reduction='mean')\n\n    adv_img = Variable(org_img.data, requires_grad=True)\n    accumulated_grad = torch.zeros_like(org_img)\n\n    for _ in range(maxiter):\n        output = target_model(adv_img)\n        true_label = output.clone()\n        true_label[true_label >= 0.5] = 1\n        true_label[true_label < 0.5] = 0\n        loss = -loss_func(output, target.float())\n\n        target_model.zero_grad()\n        if adv_img.grad is not None:\n            adv_img.grad.data.zero_()\n\n        loss.backward()\n\n        grad = adv_img.grad / (torch.norm(adv_img.grad, p=2) + 1e-10)\n        if _ > 0:\n            adaptive_alpha = alpha * (1 + torch.cosine_similarity(accumulated_grad.view(-1), grad.view(-1), dim=0)).clamp(min=0)\n        else:\n            adaptive_alpha = alpha\n\n        accumulated_grad = decay_factor * accumulated_grad + grad\n\n        perturbation = adaptive_alpha * (accumulated_grad.sign() + grad) * (1 - decay_factor * loss.item())\n        adv_img = adv_img + perturbation\n        adv_img = torch.clamp(adv_img, org_img - epsilon, org_img + epsilon)\n        adv_img = torch.clamp(adv_img, 0, 1)\n        adv_img = Variable(adv_img.data, requires_grad=True)\n\n    return adv_img",
     "objective": 323.83052,
     "other_inf": null
}