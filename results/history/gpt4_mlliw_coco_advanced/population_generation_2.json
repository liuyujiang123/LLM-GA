{
     "algorithm": "This algorithm implements a gradient ascent approach with adaptive step size and uses L-infinity normalization to create adversarial examples.",
     "code": "import torch\nfrom torch.autograd import Variable\n\ndef gen_adv_examples(self, org_img, model, target):\n    \"\"\"\n    {This algorithm implements a gradient ascent approach with adaptive step size and uses L-infinity normalization to create adversarial examples.}\n    \"\"\"\n    adv_img = Variable(org_img.data, requires_grad=True)\n    step_size = self.alpha\n    for _ in range(self.maxiter):\n        grad = self.get_gradient(adv_img, model, target)\n        grad = grad / (torch.norm(grad, p=float('inf')) + 1e-8)  # Using L-infinity normalization\n        adv_img = adv_img + step_size * grad.sign()\n        adv_img = self.clip_adv(org_img, adv_img)\n        step_size = step_size * 0.9  # Adaptive step size reduction\n\n    return adv_img",
     "objective": 559.1853,
     "other_inf": null
}