{
     "algorithm": "This algorithm employs a momentum-enhanced gradient ascent with adaptive perturbation scaling and a sinusoidal decay to generate adversarial examples efficiently.",
     "code": "import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\n# {This algorithm employs a momentum-enhanced gradient ascent with adaptive perturbation scaling and a sinusoidal decay to generate adversarial examples efficiently.}\n\ndef gen_adv_examples(org_img, target_model, target):\n    adv_img = Variable(org_img.data, requires_grad=True)\n    maxiter = 30\n    epsilon = 0.3\n    alpha = 0.002\n    g = 0\n    momentum = 0.9\n    decay_factor = 0.1\n    loss_func = nn.BCELoss(reduction='mean')\n    \n    for i in range(maxiter):\n        # Sinusoidal decay for dynamic alpha adjustment\n        adjusted_alpha = alpha * (1 + torch.sin(torch.tensor(i / maxiter * 3.141592653589793)))\n        \n        output = target_model(adv_img)\n        loss = -loss_func(output, target.float())\n        \n        target_model.zero_grad()\n        if adv_img.grad is not None:\n            adv_img.grad.data.zero_()\n        loss.backward()\n        \n        # Momentum-enhanced gradient ascent\n        g = momentum * g + (1 - decay_factor) * adv_img.grad / (torch.norm(adv_img.grad, p=2, dim=[1, 2, 3], keepdim=True) + 1e-8)\n        \n        # Update adversarial image\n        adv_img = adv_img + adjusted_alpha * g.sign()\n        \n        # Clamp the adversarial image to be within epsilon of the original image\n        adv_img = torch.clamp(adv_img, org_img - epsilon, org_img + epsilon)\n        adv_img = torch.clamp(adv_img, 0, 1)\n        \n        adv_img = Variable(adv_img.data, requires_grad=True)\n    \n    return adv_img",
     "objective": 167.01641,
     "other_inf": null
}